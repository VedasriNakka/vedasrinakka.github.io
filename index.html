<!DOCTYPE HTML>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-125946091-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-125946091-1');
  </script>

  <meta charset="UTF-8">
  <title>Vedasri Nakka</title>
  <meta name="author" content="Vedasri Nakka">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border-spacing:0px;margin:auto;">
    <tr>
      <td>
        <table style="width:100%;border-spacing:0px;margin:auto;">
          <tr>
            <td style="padding:2.5%;width:63%;vertical-align:middle;">
              <p style="text-align:center;">
                <name>Vedasri Nakka</name>
              </p>
              <p>I finished my Master‚Äôs in Computer Science in September 2024 through the Swiss Joint Master of Science in Computer Science program (University of Bern, University of Neuchatel, University of Fribourg) with a grade of 5/6 and a specialization in Data Science.</p>

              <p>My thesis focused on Contrastive Learning for Character Detection in Ancient Greek Papyri images, under the supervision of <a href="https://scholar.google.com/citations?user=-HlcweoAAAAJ&hl=en">Prof. Andreas Fischer</a>, <a href="https://www.unifr.ch/uni/de/organisation/leitung/duplicate-of-riold2/duplicate-of-duplicate-of-prof.-rolf-ingold.html">Prof. Rolf Ingold</a>, and <a href="https://scholar.google.ch/citations?user=97uiYAoAAAAJ&hl=de">Lars Vogtlin</a>.</p>

              <p>Prior to this, I earned my Bachelor‚Äôs degree in Electronic Communication Engineering from <a href="https://bvrit.ac.in/">B.V. Raju Institute of Technology</a> with 73% marks in 2016. I then spent three years at <a href="https://www.cyient.com/">Cyient</a> in Hyderabad, working on ServiceNow technology, gaining experience in administration, development, and IoT, including modules like client scripts, server-side scripts, workflows, and ITSM processes such as incident, problem, and change management.</p>

              <p>Currently, I am seeking roles in Machine Learning, AI, and LLMs. I have completed various projects in Computer Vision and Machine Learning during my master's.</p>

              <p style="text-align:center;">
                <a href="mailto:vedasri.g555@gmail.com">Email</a> &nbsp;/&nbsp;
                <a href="data/VedasriNakkaResume_v1.pdf">CV</a> &nbsp;/&nbsp;
                <a href="https://github.com/VedasriNakka">GitHub</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/vedasri-nakka-8a7865b3/">LinkedIn</a> &nbsp;/&nbsp;
                <a href="https://www.arxiv.org/abs/2409.10156">Thesis Report</a> &nbsp;/&nbsp;
                <a href="data/Thesis_final_v2.pdf">Thesis Slides</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;vertical-align:middle;">
              <a href="images/veda_image.jpg">
                <img style="width:100%;" alt="profile photo" src="images/veda_image.jpg">
              </a>
            </td>
          </tr>
        </table>

        <table style="width:100%;border-spacing:0px;margin:auto;">
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle;">
              <h2>Projects</h2>
              <p>During my master‚Äôs, my main focus was on data science-related courses and projects. I gained a strong understanding of core Machine Learning concepts and algorithms, particularly through the Pattern Recognition course, where I learned about KNN, Clustering, Bayesian classification, Support Vector Machines, and Artificial Neural Networks. To deepen my knowledge, I also took courses such as Machine Learning, Digital Humanities, and Statistical Learning, where we explored ML topics in detail.</p>

	      <p>Additionally, I read papers on machine learning during seminars and wrote scientific articles. One of the papers I co-authored in the Seminar Public Services course was published in the <em>Informatik Spektrum Journal</em>.</p>

	      <p>In the Fuzzy System course, I completed a project that utilized fuzzy logic technology to recommend personalized travel destinations by analyzing user responses. I also studied Recommendation Systems, which helped me understand how recommendation engines on websites and social media platforms function. </p>            
	    </td>
          </tr>
        </table>

        <table style="width:100%;border-spacing:0px;margin:auto;">
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <img src="images/simclr_method_image_v2.png" alt="SimCLR method image" width="240" height="200">
            </td>
            <td style="width:75%;vertical-align:middle;">
              <a href="https://www.arxiv.org/abs/2409.10156">
                <papertitle>Contrastive Learning for Character Detection in Ancient Greek Papyri</papertitle>
              </a>
              <br>
		    <a href="https://www.unifr.ch/home/en/">University of Fribourg</a>, Feb 2024 - September 2024. 
	      <br>
		    <a href="http://arxiv.org/abs/2409.10156">Thesis Report</a> and <a href="https://github.com/VedasriNakka/MS_project/tree/main">Code</a>
              <p>This project evaluates SimCLR, a contrastive learning technique, in recognizing Greek letters and compares its performance with traditional supervised models. By pretraining SimCLR on a large dataset and fine-tuning it on a smaller one, I assessed the impact of different data augmentation strategies. I also explored why supervised learning models outperformed SimCLR in this specific task. </p>
            </td>
          </tr>
        </table>

        <table style="width:100%;border-spacing:0px;margin:auto;">
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle;">
              <h2>Papers</h2>
	      <a href="https://link.springer.com/article/10.1007/s00287-024-01569-9">
              	<papertitle>A Life Engineering Perspective on Algorithms, AI, Social Media, and Quantitative Metrics</papertitle></a>
		<br>
		      Georgiana Bigea, Maria Mumtaz, Prof. Edy Portmann, Jennifer Swaminathan, <em>Vedasri Nakka</em>
                <br>
		      <a href="https://www.springerprofessional.de/informatik-spektrum/5053144">Informatik Spektrum</a>, Accepted: 2 May 2024
              <p>This paper discusses the intersection of life engineering, algorithms, AI, social media, and their impact on human life. We reviewed three influential books: Cathy O'Neil's "Weapons of Math Destruction", Kate Crawford's "Atlas of AI", and Shoshana Zuboff's "The Age of Surveillance Capitalism". Our analysis highlights the need for ethical technology that balances innovation with societal well-being. Read more in the <a href="https://link.springer.com/article/10.1007/s00287-024-01569-9">paper</a>.</p>
            </td>
          </tr>
        </table>

        <table style="width:100%;border-spacing:0px;margin:auto;">
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle;">
              <h2>Work Experience</h2>
              <p>I have three years of experience as a ServiceNow Developer at Cyient.</p>
              <ul>
                <li>Proficient in integrating ServiceNow with other platforms, including ServiceNow to ServiceNow and ServiceNow to third-party tools.</li>
                <li>Worked with modules such as update sets, notifications, web services, business rules, client scripts, reports, and script includes.</li>
                <li>Experienced in ITSM processes like incident, change, and problem management.</li>
                <li>Collaborated on workflows and automated processes using ServiceNow's Flow Designer and Workflow Editor.</li>
              </ul>
            </td>
          </tr>
        </table>

        <table style="width:100%;border-spacing:0px;margin:auto;">
          <tr>
            <td style="padding:10px;text-align:right;font-size:small;">
              <p>Credits: Webpage template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.</p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>
</html>







<!--	
<p>My overarching goal is to develop machine learning models that are both robust and privacy-aware, in the domains of safety and security-critical applications. In my current research, I concentrate on enhancing privacy aspects within Large Language Models (LLMs) and within the context of Federated Learning. Additionally, I explore AutoML techniques to optimize hyperparameters in the Federated Learning setting.</p>

<p>During my doctoral studies, my primary focus was on comprehending the limitations of deep neural networks concerning out-of-distribution and adversarial scenarios, with the aim of improving robustness against adversarial domain shifts. My research encompassed areas such as interpretable models, transfer-based black-box attacks, attack detection, adversarial defenses, anomaly detection, and the evaluation of disentangled representations. While at VITA, my research delved into various projects related to human-pose estimation, tracking, and re-identification, with a particular application in the field of team sports analytics.</p>

<p>I am enthusiastic about collaborating with motivated students and researchers who share an interest in Adversarial Machine Learning. If my research background aligns with your interests, please feel free to reach out to me via email.</p>
-->





<!--
<li><a href="https://www.mitacs.ca/en/programs/globalink/globalink-research-internship">MITACS Summer Research Scholarship</a> to conduct research at the University of Alberta</li>
<li><a href="https://science.uq.edu.au/student-support/scholarships/undergraduate-scholarships/uq-summer-research-program">University of Queensland Summer Research Scholarship</a> to support the internship at <a href="https://cai.centre.uq.edu.au/">Center for Advanced Imaging Institute</a></li>
<li><a href="https://www.epfl.ch/education/phd/edic-computer-and-communication-sciences/edic-for-phd-students/">EDIC PhD Fellowship</a> for pursuing the first year of doctoral studies at EPFL</li>
</ul>

	
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/disentanglement.jpg" alt="teaser" width="240" height="200">
            </td>
            <td width="75%" valign="middle">
                                          <a href="data/posepaper.pdf">

                <papertitle>Understanding Pose and Appearance Disentanglement in 3D Human Pose Estimation</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>Preprint</em>, 2022
               <br>
              <a href="data/posepaper.pdf">Paper</a>
              <br>
              <p>Our analyses show that disentanglement in the three state-of-the-art disentangled representation learning frameworks is far from complete,
              and that their pose codes contain significant appearance information</p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pami.jpg" alt="teaser" width="240" height="120">
            </td>
            <td width="75%" valign="middle">
                            <a href="data/pami.pdf">

                <papertitle>Universal, Transferable Adversarial Attacks for Visual Object Trackers</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <a href="data/pami.pdf">Paper</a>
              <br>
              <em>Adversarial Robustness Workshop, European Conference on Computer Vision (ECCV)</em>, 2022
              <p>We propose to learn to generate a single perturbation from the
              object template only, that can be added to every search image and still successfully fool the tracker for the entire video. As a
              consequence, the resulting generator outputs perturbations that are quasi-independent of the template, thereby making them universal
              perturbations.</p>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/neurips.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://proceedings.neurips.cc/paper/2021/hash/7486cef2522ee03547cfb970a404a874-Abstract.html">
                <papertitle>Learning Transferable Adversarial Perturbations</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>Neural Information and Processing Systems (NeurIPS)</em>, 2021
              <br>
              <a href="https://proceedings.neurips.cc/paper/2021/hash/7486cef2522ee03547cfb970a404a874-Abstract.html">arXiv</a> /
              <a href="https://github.com/krishnakanthnakka/Transferable_Perturbations">code</a>
              <br>
              <p>We show that generators trained with mid-level feature separation loss transfers significantly better in cross-model, cross-domain and cross-task setting</p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/accv.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/ACCV2020/html/Nakka_Towards_Robust_Fine-grained_Recognition_by_Maximal_Separation_of_Discriminative_Features_ACCV_2020_paper.html">
                <papertitle>Towards Robust Fine-grained Recognition by Maximal Separation of Discriminative Features</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em> Asian Conference on Computer Vision (ACCV)</em>, 2020
              <br>
               <a href="https://arxiv.org/abs/2006.06028">arXiv</a> /
              <a href="https://github.com/krishnakanthnakka/RobustFineGrained/">code</a> /
              <a href="data/slides_accv.pdf">Slides</a>
              <br>
              <p>We improve the robustness by introducing an attention-based regularization mechanism that maximally separates the latent features of discriminative regions of different classes
              while minimizing the contribution of the non-discriminative regions to the final class prediction.</p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/eccv2020.jpg" alt="teaser" width="240" height="170">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500596.pdf">
                <papertitle>Indirect Local Attacks for Context-aware Semantic Segmentation Networks</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>European Conference on Computer Vision  (ECCV)</em>, 2020 <strong>[Spotlight]</strong>
              <br>
               <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500596.pdf">arXiv</a> /
              <a href="https://github.com/krishnakanthnakka/Indirectlocalattacks/">code</a> /
              <a href="data/slides_eccv.pdf">Slides</a>

              <br>
              <p> We show that the resulting networks are sensitive not only to global attacks, where perturbations affect the entire input image, but also to indirect local attacks
              where perturbations are confined to a small image region that does not overlap with the area that we aim to fool. </p>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iccv.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1904.07595">
                <papertitle>Detecting the Unexpected via Image Resynthesis</papertitle>
              </a>
              <br>
              <a href="https://adynathos.net/">Krzysztof Lis</a>, <strong>Krishna Kanth Nakka</strong>,  <a href="https://people.epfl.ch/pascal.fua/bio?lang=en">Pascal Fua</a> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em> International Conference on Computer Vision (ICCV) </em>, 2019
              <br>
               <a href="https://arxiv.org/abs/1904.07595">arXiv</a> /
              <a href="https://github.com/cvlab-epfl/detecting-the-unexpected/">code</a> /
              <a href="data/DetectingTheUnexpected_Poster.pdf">Poster</a>

              <br>
              <p> We rely on the intuition that the network will produce spurious labels in regions depicting unexpected anomaly objects.
              Therefore, resynthesizing the image from the resulting semantic map will yield significant appearance differences with respect to the input image which we detect through  an auxiliary network</p>

            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iccvw.jpg" alt="teaser" width="240" height="200">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1901.02229">
                <papertitle>Interpretable BoW Networks for Adversarial Example Detectio</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>Explainable and Interpretable AI workshop, ICCV</em>, 2018  <strong>[Oral]</strong>
              <br>
               <a href="https://arxiv.org/abs/1901.02229">arXiv</a> /
              <a href="data/iccvw_slides.pdf">Slides</a>

              <br>
              <p> We build upon the intuition that, while adversarial samples look very similar to real images, to produce incorrect predictions, they should activate
                codewords with a significantly different visual representation.
              We therefore cast the adversarial example detection problem as that of comparing the input image with the most highly activated visual codeword.</p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bmvc.jpg" alt="teaser" width="240" height="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1805.05389">
                <papertitle>Deep Attentional Structured Representation Learning for Visual Recognition</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>British Media Vision Conference (BMVC)</em>, 2018
              <br>
               <a href="https://arxiv.org/abs/1805.05389">arXiv</a> /
                              <a href="data/BMVC2018_Poster.pdf">Poster</a>

                             <br>
              <p> we introduce an attentional structured representation learning framework that incorporates an image-specific attention mechanism within the aggregation process. </p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/eccv2016.jpg" alt="teaser" width="240" height="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1609.07727">
                <papertitle>Deep learning based fence segmentation and removal from an image using a video sequence</papertitle>
              </a>
              <br>
               SankarGanesh Jonna, <strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ"> Rajiv Ranjan Sahay</a>
              <br>
              <em>International Workshop on Video Segmentation, ECCV</em>, 2016 <strong>[Oral]</strong>
              <br>
               <a href="https://arxiv.org/abs/1609.07727">arXiv</a> /
              <a href="data/Slides_ECCV2016.pdf">Slides</a>
              <br>
              <p>  We use  knowledge of spatial locations of fences to subsequently estimate  occlusion-aware optical flow. We then fuse the occluded information from neighbouring frames
              by solving inverse problem of denoising</p>

            </td>
          </tr>


           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/josaa.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://opg.optica.org/josaa/viewmedia.cfm?uri=josaa-33-10-1917">
                <papertitle>Detection and removal of fence occlusions in an image using a video of the static/dynamic scene</papertitle>
              </a>
              <br>
               SankarGanesh Jonna, <strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ"> Rajiv Ranjan Sahay</a>
              <br>
              <em>Journal of the Optical Society of America A (JOSA A) </em>, 2016
              <br>
               <a href="https://opg.optica.org/josaa/viewmedia.cfm?uri=josaa-33-10-1917">arXiv</a> /
              <a href="data/JOSA.pdf">PDF</a>
              <br>
              <p> Our approach of defencing is as follows: (i) detection of spatial locations of fences/occlusions in the frames of the video, (ii) estimation
              of relative motion between the observations, and (iii) data fusion to fill in occluded pixels in the reference image. We assume the de-fenced image as a Markov random
            field and obtain its maximum a posteriori estimate by solving the corresponding inverse problem. </p>

            </td>
          </tr>


           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/acpr.jpg" alt="teaser" width="240" height="200">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/7486506">
                <papertitle>My camera can see through fences: A deep learning approach for image de-fencing</papertitle>
              </a>
              <br>
               SankarGanesh Jonna, <strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ"> Rajiv Ranjan Sahay</a>
              <br>
              <em>Asian Conference on Pattern Recognition (ACPR),  </em>, 2015
              <br>
               <a href="https://ieeexplore.ieee.org/abstract/document/7486506">arXiv</a> /
              <a href="data/My_camera_can_see_through_fences_A_deep_learning_approach_for_image_de-fencing.pdf">PDF</a> /
              <a href="data/acpr_poster.pdf">Poster</a>
               <p> We propose a semi-automated de-fencing algorithm using a video of the dynamic scene. The inverse problem offence removal is solved using split Bregman
                technique assuming total variation of the de-fenced image as the regularization constraint.
              </p>

              <br>

            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/globalsip.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/7032076">
                <papertitle>3D-to-2D mapping for user interactive segmentation of human leg muscles from MRI data</papertitle>
              </a>
              <br>
              Nilanjan Ray, Satarupa Mukherjee, <strong>Krishna Kanth Nakka</strong>, Scott T. Acton, Silvia S. Blanker
              <br>
              <em>Signal and Information Processing, GlobalSIP</em>, 2014
              <br>
               <a href="https://ieeexplore.ieee.org/abstract/document/7032076">arXiv</a> /
              <a href="data/3D-to-2D_mapping_for_user_interactive_segmentation_of_human_leg_muscles_from_MRI_data.pdf">PDF</a>
              <br>
              <p>
              We proposing a framework for user interactive segmentation of MRI of human leg muscles built upon the the strategy of bootstrapping with minimal supervision.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/nus1.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://pubs.rsc.org/en/content/articlepdf/2014/cp/c4cp02172j">
                <papertitle>Non-uniform sampling in EPR: optimizing data acquisition for Hyscore spectroscopy</papertitle>
              </a>
              <br>
               <strong>Krishna Kanth Nakka</strong> Y. A. Tesiram, I. M. Brereton,  M. Mobli and J. R. Harmer
              <br>
              <em>Physical Chemistry Chemical Physics (PCCP)</em>, 2014
              <br>
               <a href="https://pubs.rsc.org/en/content/articlepdf/2014/cp/c4cp02172j">Paper</a> /
                <a href="data/pccp_main.pdf">PDF</a> /
                <a href="data/pccp_supp.pdf">Supp</a>
              <br>
              <p>We show through non-linear sampling scheme with maximum entropy reconstruction technique in HYSCORE, the experimental times can be shortened by
              approximately an order of magnitude as compared to conventional linear sampling with negligible loss of information
              </p>

            </td>
          </tr>
        </tbody>
      </table>
-->


<!--      

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Scholarships</heading>
        <p>I'm deeply grateful for the generous scholarships I received throughout my academic journey. Some of these scholarships include:</p>
        <ul>

          <li><a href="https://www.mitacs.ca/en/programs/globalink/globalink-research-internship">MITACS Summer Research Scholarship</a> to conduct research at the University of Alberta</li>
        <li><a href="https://science.uq.edu.au/student-support/scholarships/undergraduate-scholarships/uq-summer-research-program">University of Queensland Summer Research Scholarship</a> to support the internship at <a href="https://cai.centre.uq.edu.au/">Center for Advanced Imaging Institute</a></li>
        <li><a href="https://www.epfl.ch/education/phd/edic-computer-and-communication-sciences/edic-for-phd-students/">EDIC PhD Fellowship</a> for pursuing the first year of doctoral studies at EPFL</li>
        </ul>


      </td>
    </tr>
  </tbody>
</table>

  
	

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Reviewer</heading>

              <p>I have peer-reviewed more than 50 articles. Some of them include:</p>
<ul>
  <li>Reviewer at Transactions on Pattern Analysis and Machine Intelligence, 2019, 2023</li>
  <li>Reviewer at Neural Information Processing Systems, NeurIPS 2021, 2022, 2023</li>
  <li>Reviewer at Computer Vision and Pattern Recognition, CVPR 2023</li>
  <li>Reviewer at International Conference on Computer Vision and Pattern Recognition, ICCV 2023</li>
  <li>Reviewer at International Conference on Machine Learning, ICML 2023</li>
  <li>PC Member and Reviewer at New Frontiers in Machine Learning, International Conference on Machine Learning, ICML 2023</li>
  <li>Reviewer at British Machine Vision Conference, BMVC 2023</li>
  <li>Reviewer at Winter Application for Computer Vision Conference, WACV 2019, 2024</li>
  <li>Reviewer at International Conference on Artificial Intelligence and Statistics (AISTATS) 2024</li>
  <li>Reviewer at International Conference on Learning Representations, ICLR 2024</li>
</ul>


            
            </td>
          </tr>
        </tbody>
      </table>

 <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Outside research</heading>

<p>Apart from work, I spend time with lakes. Thanks for visiting this page. I leave you with a thought that always lingers in my mind by Sirivennela gaaru:</p>

<blockquote>
  <p>‡∞®‡±Å‡∞µ‡±ç‡∞µ‡±Å ‡∞§‡∞ø‡∞®‡±á ‡∞™‡±ç‡∞∞‡∞§‡∞ø ‡∞í‡∞ï ‡∞Æ‡±Ü‡∞§‡±Å‡∞ï‡±Å ‡∞à ‡∞∏‡∞Ç‡∞ò‡∞Ç ‡∞™‡∞Ç‡∞°‡∞ø‡∞Ç‡∞ö‡∞ø‡∞Ç‡∞¶‡∞ø, ‡∞ó‡∞∞‡±ç‡∞µ‡∞ø‡∞Ç‡∞ö‡±á ‡∞à ‡∞®‡±Ä ‡∞¨‡±ç‡∞∞‡∞§‡±Å‡∞ï‡±Å ‡∞à ‡∞∏‡∞Æ‡∞æ‡∞ú‡∞Æ‡±á ‡∞Æ‡∞≤‡∞ø‡∞ö‡∞ø‡∞Ç‡∞¶‡∞ø ...<br>
  ‡∞ã‡∞£‡∞Ç ‡∞§‡±Ä‡∞∞‡±ç‡∞ö‡±Å ‡∞§‡∞∞‡±Å‡∞£‡∞Ç ‡∞µ‡∞∏‡±ç‡∞§‡±á ‡∞§‡∞™‡±ç‡∞™‡∞ø‡∞Ç‡∞ö‡±Å‡∞ï‡±Å ‡∞™‡±ã‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞µ‡∞æ, ‡∞§‡±Ü‡∞™‡±ç‡∞™ ‡∞§‡∞ó‡∞≤‡∞™‡±Ü‡∞ü‡±ç‡∞ü‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞µ‡∞æ ‡∞è‡∞∞‡±Å ‡∞¶‡∞æ‡∞ü‡∞ó‡∞æ‡∞®‡±á ...</p>
</blockquote>

<blockquote>
  <p>Loosely translates to:</p>
</blockquote>

<blockquote>
  <p>Every single grain you eat is made by this society, And this life of which you are so proud of, is shaped by the society, And when the time comes for a payback, Are you escaping away? Will you burn the boat once you cross the stream?</p>
</blockquote>

<blockquote>
  <p>Quoting Audrey, "Nothing is more important than empathy for another human being‚Äôs suffering. Nothing‚Äînot career, not wealth, not intelligence, certainly not status. We have to feel for one another if we‚Äôre going to survive with dignity."</p>
</blockquote>



  </td>
          </tr>
        </tbody>
      </table>


-->
